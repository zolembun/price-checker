import streamlit as st
import pandas as pd
import google.generativeai as genai
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import urllib.parse
from datetime import datetime
import re
import json
import time

# ---------------------------------------------------------
# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î ‡∏´‡πâ‡∏≤‡∏°‡∏¢‡πâ‡∏≤‡∏¢)
# ---------------------------------------------------------
st.set_page_config(page_title="‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤ & AI", page_icon="üí∞", layout="wide")

# ---------------------------------------------------------
# 2. ‡∏£‡∏∞‡∏ö‡∏ö Login
# ---------------------------------------------------------
def check_password():
    def password_entered():
        if st.session_state["password"] == st.secrets["app_password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        st.header("üîí ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
        st.text_input("‡πÉ‡∏™‡πà‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", type="password", on_change=password_entered, key="password")
        return False
    elif not st.session_state["password_correct"]:
        st.header("üîí ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
        st.text_input("‡πÉ‡∏™‡πà‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", type="password", on_change=password_entered, key="password")
        st.error("‚ùå ‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        return False
    else:
        return True

if not check_password():
    st.stop()

# ---------------------------------------------------------
# 3. CSS Styling
# ---------------------------------------------------------
st.markdown("""
<style>
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô */
    .cost-box { 
        background-color: #ffebee; padding: 15px; border-radius: 10px; 
        border: 2px solid #ef5350; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .price-value-cost { font-size: 42px !important; font-weight: 900; color: #c62828; line-height: 1.2;}
    
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≤‡∏¢ */
    .selling-box { 
        background-color: #e8f5e9; padding: 15px; border-radius: 10px; 
        border: 2px solid #66bb6a; text-align: center;
    }
    .price-value-sell { font-size: 42px !important; font-weight: 900; color: #2e7d32; line-height: 1.2;}
    
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• */
    .info-box {
        background-color: #f8f9fa; padding: 15px; border-radius: 10px; border: 1px solid #ddd;
    }
    
    /* ‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î */
    div.stButton > button { width: 100%; border-radius: 8px; font-weight: bold; height: 3em; }
    
    /* ‡∏ã‡πà‡∏≠‡∏ô Header */
    header {visibility: hidden;}
    
    /* Status Widget */
    .stStatusWidget { border-radius: 10px; }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# 4. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Services
# ---------------------------------------------------------
@st.cache_resource
def init_services():
    try:
        service_account_info = st.secrets["gcp_service_account"]
        gemini_key = st.secrets["gemini_api_key"]
        
        scopes = ['https://www.googleapis.com/auth/spreadsheets', 
                  'https://www.googleapis.com/auth/drive.metadata.readonly']
        creds = Credentials.from_service_account_info(service_account_info, scopes=scopes)
        
        sheets_service = build('sheets', 'v4', credentials=creds)
        drive_service = build('drive', 'v3', credentials=creds)
        
        # Config Gemini
        genai.configure(api_key=gemini_key)
        
        # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà Error: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∏‡πà‡∏ô Latest
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-lite-preview-02-05')
        except:
            # Fallback ‡∏ñ‡πâ‡∏≤ latest ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠ Pro
            model = genai.GenerativeModel('gemini-2.0-flash-lite-preview-02-05')
        
        return sheets_service, drive_service, model
    except Exception as e:
        st.error(f"Config Error: {e}")
        return None, None, None

sheets_svc, drive_svc, ai_model = init_services()
if not sheets_svc: st.stop()

try:
    SHEET_URL = st.secrets["sheet_url"]
    SPREADSHEET_ID = SHEET_URL.split('/d/')[1].split('/')[0]
except:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö sheet_url ‡πÉ‡∏ô Secrets")
    st.stop()

# ---------------------------------------------------------
# 5. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ---------------------------------------------------------
@st.cache_data(ttl=600)
def load_data_master():
    try:
        # Metadata
        file_meta = drive_svc.files().get(fileId=SPREADSHEET_ID, fields="name, modifiedTime").execute()
        file_name = file_meta.get('name')
        dt = datetime.strptime(file_meta.get('modifiedTime'), "%Y-%m-%dT%H:%M:%S.%fZ")
        last_update = dt.strftime("%d/%m/%Y %H:%M ‡∏ô.")

        # Main Data
        res_main = sheets_svc.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID, range="A:H").execute()
        vals_main = res_main.get('values', [])
        
        if vals_main:
            df_main = pd.DataFrame(vals_main[1:], columns=vals_main[0])
            for col in ['‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å']:
                if col in df_main.columns:
                    df_main[col] = pd.to_numeric(df_main[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
        else:
            df_main = pd.DataFrame()

        # AI Memory Data
        try:
            # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå F (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ AI_Kind)
            res_mem = sheets_svc.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:F").execute()
            vals_mem = res_mem.get('values', [])
            
            # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏û‡∏¥‡πà‡∏° AI_Kind ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            cols_mem = ['SKU', 'AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', 'AI_Kind']

            if vals_mem and len(vals_mem) > 1:
                headers = vals_mem[0]
                rows = vals_mem[1:]
                # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÅ‡∏´‡∏ß‡πà‡∏á
                fixed_rows = [r + [None]*(len(headers)-len(r)) for r in rows]
                df_mem = pd.DataFrame(fixed_rows, columns=headers)
                
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå AI_Kind ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                if 'AI_Kind' not in df_mem.columns:
                    df_mem['AI_Kind'] = ''
            else:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÅ‡∏ö‡∏ö‡∏°‡∏µ AI_Kind ‡∏£‡∏≠‡πÑ‡∏ß‡πâ
                df_mem = pd.DataFrame(columns=cols_mem)
        except Exception as e:
            # ‡∏Å‡∏£‡∏ì‡∏µ Error ‡∏Å‡πá‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ AI_Kind ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
            print(f"Load Mem Error: {e}")
            df_mem = pd.DataFrame(columns=['SKU', 'AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', 'AI_Kind'])

        return df_main, df_mem, file_name, last_update

    except Exception as e:
        st.error(f"Load Data Error: {e}")
        return pd.DataFrame(), pd.DataFrame(), "Error", "-"

def append_to_sheet(data_values):
    body = {'values': data_values}
    try:
        sheets_svc.spreadsheets().values().append(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:A", 
            valueInputOption="USER_ENTERED", body=body
        ).execute()
        return True
    except Exception as e: 
        st.error(f"Save Error: {e}")
        return False

def overwrite_memory_sheet(df_new_mem):
    try:
        # 1. ‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô List ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô empty string ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô Error
        df_new_mem = df_new_mem.fillna('') 
        values = [df_new_mem.columns.tolist()] + df_new_mem.values.tolist()
        
        # 2. ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô AI_Memory
        sheets_svc.spreadsheets().values().clear(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:E"
        ).execute()

        # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏•‡∏á‡πÑ‡∏õ
        body = {'values': values}
        sheets_svc.spreadsheets().values().update(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A1",
            valueInputOption="USER_ENTERED", body=body
        ).execute()
        return True
    except Exception as e:
        st.error(f"Cleanup Error: {e}")
        return False

@st.cache_data(ttl=600)
def merge_data(df_main, df_mem):
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
    if df_mem.empty: return df_main.copy()
    
    # Copy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
    df_main_c = df_main.copy()
    df_mem_c = df_mem.copy()
    
    # üî• ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç 1: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ + ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà + ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Normalize)
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ "sku01" ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö "SKU01" ‡∏´‡∏£‡∏∑‡∏≠ " SKU01 "
    df_main_c['join_key'] = df_main_c['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().str.upper()
    df_mem_c['join_key'] = df_mem_c['SKU'].astype(str).str.strip().str.upper()
    
    # 2. ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà (Merge) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (join_key)
    merged = pd.merge(df_main_c, df_mem_c, on='join_key', how='left')
    
    # 3. ‡∏ñ‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ñ‡πâ‡∏≤ AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NaN)
    cols_to_fix = ['AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', 'AI_Kind'] 
    for col in cols_to_fix:
        if col in merged.columns:
            # fillna('') ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏°‡πà error ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            merged[col] = merged[col].fillna('').astype(str)
    
    # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢ (join_key) ‡∏ó‡∏¥‡πâ‡∏á
    if 'join_key' in merged.columns:
        del merged['join_key']
            
    return merged
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (Force JSON + Debug)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏ô‡∏±‡∏Å‡πÅ‡∏Å‡∏∞‡∏£‡∏≠‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡∏û)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏ô‡∏±‡∏Å‡πÅ‡∏Å‡∏∞‡∏£‡∏≠‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡∏û - ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î Prompt)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏™‡∏∏‡∏ô - ‡πÅ‡∏Å‡πâ Error JSON)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô Ultra-Safe: ‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÇ‡∏´‡∏°‡∏î DEBUG: ‡πÅ‡∏™‡∏î‡∏á Error ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏à‡∏∞‡πÜ)
# ---------------------------------------------------------
def ask_gemini_extract(names):
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡πà‡∏≤ Default ‡πÑ‡∏ß‡πâ
    default_list = []
    for _ in names:
        default_list.append({
            "AI_Brand": "Unknown", "AI_Type": "Other", 
            "AI_Kind": "", "AI_Spec": "-", "AI_Tags": ""
        })

    if not names: return []

    # Prompt ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô
    prompt = f"""
    Extract product info from this list:
    {json.dumps(names, ensure_ascii=False)}

    Return JSON Array with these keys:
    - AI_Brand
    - AI_Type (Category in Thai)
    - AI_Kind (Sub-type in Thai e.g. 1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π, ‡∏ù‡∏≤‡∏ö‡∏ô. If unknown use "")
    - AI_Spec (Capacity/Size)
    - AI_Tags

    Response Format: JSON Array ONLY. No Markdown.
    """
    
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI
        response = ai_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            )
        )
        
        txt = response.text.strip()
        
        # ---------------------------------------------------
        # üïµÔ∏è‚Äç‚ôÄÔ∏è ‡∏™‡πà‡∏ß‡∏ô DEBUG: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡πÜ ‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        # ---------------------------------------------------
        with st.expander(f"üîç X-Ray: ‡∏î‡∏π‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà AI ‡∏ï‡∏≠‡∏ö‡∏°‡∏≤ (Batch ‡∏ô‡∏µ‡πâ)", expanded=True):
            if not txt:
                st.error("‚ùå AI ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ (Empty Response)")
            else:
                st.code(txt, language='json') # ‡πÇ‡∏ä‡∏ß‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà AI ‡∏™‡πà‡∏á‡∏°‡∏≤
        # ---------------------------------------------------

        # ‡∏•‡πâ‡∏≤‡∏á Markdown ‡∏≠‡∏≠‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        txt_clean = re.sub(r"```json|```", "", txt).strip()
        
        try:
            data = json.loads(txt_clean)
        except json.JSONDecodeError as json_err:
            st.error(f"üí• ‡πÅ‡∏õ‡∏•‡∏á JSON ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {json_err}")
            return default_list

        # ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        normalized_data = []
        for item in data:
            new_item = {
                "AI_Brand": item.get("AI_Brand") or "Unknown",
                "AI_Type": item.get("AI_Type") or "Other",
                "AI_Kind": item.get("AI_Kind") or "", 
                "AI_Spec": item.get("AI_Spec") or "-",
                "AI_Tags": item.get("AI_Tags") or ""
            }
            if isinstance(new_item["AI_Tags"], list):
                new_item["AI_Tags"] = ", ".join(new_item["AI_Tags"])
                
            normalized_data.append(new_item)
            
        return normalized_data

    except Exception as e:
        # ‡πÅ‡∏™‡∏î‡∏á Error ‡∏ï‡∏±‡∏ß‡πÅ‡∏î‡∏á‡πÜ ‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        st.error(f"‚ò†Ô∏è Critical Error: {e}")
        return default_list
def ask_gemini_filter(query, columns):
    # Prompt ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏°‡πà: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡πÄ‡∏õ‡∏Ñ (Spec Range)
    prompt = f"""
    Role: ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Search Engine ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ "{query}" ‡πÄ‡∏õ‡πá‡∏ô JSON Filter
    Columns: {columns}
    
    Instruction (Strict Rules):
    1. **Category/Kind**: ‡πÅ‡∏¢‡∏Å AI_Type (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó) ‡∏Å‡∏±‡∏ö AI_Kind (‡∏ä‡∏ô‡∏¥‡∏î) ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    2. **Price Logic**: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ lte (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô), gte (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà)
    
    3. **Decimal Range Strategy (Spec Only)**: 
       - ‡∏ñ‡πâ‡∏≤ User ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏ô‡∏≤‡∏î/‡∏™‡πÄ‡∏õ‡∏Ñ (‡πÄ‡∏ä‡πà‡∏ô "5.5 - 6 ‡∏Ñ‡∏¥‡∏ß", "9000-12000 btu") 
       - **‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ operator 'gte' (>=) ‡πÅ‡∏•‡∏∞ 'lte' (<=) ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå AI_Spec**
       - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "5.5 - 6 ‡∏Ñ‡∏¥‡∏ß" -> 
         {{ "column": "AI_Spec", "operator": "gte", "value": "5.5" }},
         {{ "column": "AI_Spec", "operator": "lte", "value": "6.0" }}
       - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ 'contains' ‡∏´‡∏£‡∏∑‡∏≠ 'in' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡πÄ‡∏õ‡∏Ñ
       
    4. **Single Number Spec**: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏ä‡πà‡∏ô "5 ‡∏Ñ‡∏¥‡∏ß") ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 'contains' ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    
    Output Format (JSON):
    {{
        "filters": [ ... ],
        "sort_order": "asc"
    }}
    """
    try:
        res = ai_model.generate_content(prompt, generation_config=genai.types.GenerationConfig(response_mime_type="application/json"))
        return json.loads(res.text.strip())
    except: return None
def clean_text(text):
    # üëá ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ (Indentation) ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
    if not text: return ""
    return re.sub(r'[^a-zA-Z0-9]', '', str(text)).lower()
# ---------------------------------------------------------
# 6. MAIN APP UI (TABS)
# ---------------------------------------------------------

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df_main, df_mem, file_name, last_update = load_data_master()

st.title("üí∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ & AI")
st.caption(f"üìÇ ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {file_name} | üïí ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {last_update}")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á TAB ‡πÄ‡∏°‡∏ô‡∏π
tab1, tab2 = st.tabs(["üè† ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (Code/Name)", "ü§ñ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (AI Search)"])

# =========================================================
# TAB 1: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
# =========================================================
with tab1:
    st.info("üí° ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏π‡πâ '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô' ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô")
    
    query1 = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô", placeholder="‡πÄ‡∏ä‡πà‡∏ô rt20, parsr5lae (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ç‡∏µ‡∏î)", key="search_tab1")
    
    if query1:
        match_index = -1
        found_by = ""
        
        query_clean = clean_text(query1)
        sku_clean_series = df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).apply(clean_text)
        desc_clean_series = df_main['‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).apply(clean_text)
        
        sku_matches = df_main[sku_clean_series.str.contains(query_clean, na=False)]
        if not sku_matches.empty:
            match_index = sku_matches.index[0]
            found_by = "‚ö° ‡πÄ‡∏à‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"
        else:
            desc_matches = df_main[desc_clean_series.str.contains(query_clean, na=False)]
            if not desc_matches.empty:
                match_index = desc_matches.index[0]
                found_by = "üîé ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
            else:
                keywords = list(filter(None, re.split(r'[^a-zA-Z0-9]', query1)))
                if not keywords: keywords = [query1]
                # ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö
                candidates = df_main[df_main.astype(str).apply(lambda x: any(k.lower() in ' '.join(x).lower() for k in keywords), axis=1)]
                
                if candidates.empty: search_pool = df_main.sample(min(len(df_main), 15))
                else: search_pool = candidates.head(30)
                
                prod_str = search_pool[['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤']].to_string(index=True)
                with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡∏∞‡∏•‡∏≤‡∏¢‡πÅ‡∏ó‡∏á...'):
                    try:
                        res = ai_model.generate_content(f"‡∏´‡∏≤ index ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö '{query1}' ‡∏à‡∏≤‡∏Å:\n{prod_str}\n‡∏ï‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç index. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ -1")
                        match_index = int(res.text.strip())
                        found_by = "ü§ñ AI ‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö"
                    except: match_index = -1

        if match_index != -1 and match_index in df_main.index:
            item = df_main.loc[match_index]
            cost = item.get('‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', 0)
            stock = item.get('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å', 0)
            mid = item.get('‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '-')
            name = item.get('‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '-')
            brand = item.get('‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠', '-')

            st.success(f"{found_by}: {name}")
            
            target_margin = 12
            sell_price = cost * (1 + (target_margin/100))
            profit = sell_price - cost

            c1, c2, c3 = st.columns([1.3, 1.3, 1])
            with c1:
                st.markdown(f"""<div class="cost-box"><div style="color:#555;font-weight:bold;">üî¥ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô</div><div class="price-value-cost">{cost:,.0f}</div></div>""", unsafe_allow_html=True)
            with c2:
                st.markdown(f"""<div class="selling-box"><div style="color:#555;font-weight:bold;">üü¢ ‡∏Ç‡∏≤‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (+{target_margin}%)</div><div class="price-value-sell">{sell_price:,.0f}</div><div style="color:#1b5e20;">‡∏Å‡∏≥‡πÑ‡∏£ {profit:,.0f} ‡∏ö‡∏≤‡∏ó</div></div>""", unsafe_allow_html=True)
            with c3:
                st.markdown(f"""<div class="info-box"><b>üÜî ‡∏£‡∏´‡∏±‡∏™:</b> {mid}<br><b>üì¶ ‡∏™‡∏ï‡πâ‡∏≠‡∏Å:</b> {stock}<br><b>üè∑Ô∏è ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠:</b> {brand}</div>""", unsafe_allow_html=True)
                
                st.write("")
                g_q = urllib.parse.quote(name)
                st.link_button("üåê ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ/‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Google", f"https://www.google.com/search?q={g_q}", use_container_width=True)

            st.divider()
            with st.expander("‡∏î‡∏π‡∏ï‡∏≤‡∏£‡∏≤‡∏á Margin (3% - 30%)", expanded=True):
                margins = [3, 5, 8, 10, 12, 15, 18, 25, 30]
                p_data = [{"‡∏Å‡∏≥‡πÑ‡∏£ %": f"{m}%", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≤‡∏¢": f"{cost*(1+m/100):,.0f}", "‡∏Å‡∏≥‡πÑ‡∏£ (‡∏ö‡∏≤‡∏ó)": f"{(cost*(1+m/100))-cost:,.0f}"} for m in margins]
                st.dataframe(pd.DataFrame(p_data), hide_index=True, use_container_width=True)

            st.divider()
            st.subheader("üõí ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏Ç‡πà‡∏á (Hot Search)")
            default_search = re.sub(r'[\u0E00-\u0E7F]', '', str(mid)).strip('-').strip()
            final_kw = st.text_input("üéØ ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:", value=default_search, key=f"hot_kw_{match_index}")
            
            if final_kw:
                enc = urllib.parse.quote(final_kw.strip())
                stores = [
                    {"name": "Shopee", "url": f"https://shopee.co.th/search?keyword={enc}"},
                    {"name": "Lazada", "url": f"https://www.lazada.co.th/catalog/?q={enc}"},
                    {"name": "HomePro", "url": f"https://www.homepro.co.th/search?q={enc}"},
                    {"name": "PowerBuy", "url": f"https://www.powerbuy.co.th/th/search/{enc}"},
                    {"name": "ThaiWatsadu", "url": f"https://www.thaiwatsadu.com/th/search/{enc}"},
                    {"name": "Big C", "url": f"https://www.bigc.co.th/search?q={enc}"},
                    {"name": "Global", "url": f"https://globalhouse.co.th/search?keyword={enc}"},
                    {"name": "Makro", "url": f"https://www.makro.pro/c/search?q={enc}"},
                    {"name": "Dohome", "url": f"https://www.dohome.co.th/search?q={enc}"}
                ]
                
                cols = st.columns(2)
                for i, s in enumerate(stores):
                    with cols[i%2]: st.link_button(f"üîç {s['name']}", s['url'], use_container_width=True)
        else:
            if query1: st.warning(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤: '{query1}'")

# =========================================================
# TAB 2: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ AI
# =========================================================
with tab2:
    st.info("üí° ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô '‡∏ó‡∏µ‡∏ß‡∏µ Samsung ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏´‡∏°‡∏∑‡πà‡∏ô', '‡πÅ‡∏≠‡∏£‡πå inverter'")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
    processed_skus = df_mem['SKU'].astype(str).str.strip().tolist() if not df_mem.empty else []
    new_items_df = df_main[~df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().isin(processed_skus)]
    new_count = len(new_items_df)
    
    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏≠‡∏á AI ---
    with st.expander(f"‚öôÔ∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏≠‡∏á AI ({len(df_mem)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß)"):
        c_a1, c_a2 = st.columns([3, 1])
        c_a1.write(f"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: **{new_count}** ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏≠‡∏ô AI
        if new_count > 0:
            if c_a2.button("üöÄ ‡∏™‡∏≠‡∏ô AI ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ô‡∏µ‡πâ", type="primary"):
                with st.status("ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ...", expanded=True) as status:
                    
                    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏ô‡∏¥‡∏î (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
                    if '‡∏ä‡∏ô‡∏¥‡∏î' not in new_items_df.columns:
                        new_items_df['‡∏ä‡∏ô‡∏¥‡∏î'] = ''
                        
                    # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ä‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏¥‡∏°)
                    to_proc = new_items_df[['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏ä‡∏ô‡∏¥‡∏î']].rename(
                        columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤':'SKU', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤':'Name', '‡∏ä‡∏ô‡∏¥‡∏î':'Original_Kind'}
                    ).to_dict('records')

                    BATCH = 30
                    res_save = []
                    total_batches = (len(to_proc) // BATCH) + 1
                    
                    for i in range(0, len(to_proc), BATCH):
                        chunk = to_proc[i:i+BATCH]
                        status.write(f"Batch {(i//BATCH)+1}/{total_batches} ({len(chunk)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)...")
                        
                        # ‡∏£‡∏ß‡∏°‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ä‡∏ô‡∏¥‡∏î ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
                        names_for_ai = [f"{x['Name']} {x['Original_Kind']}" for x in chunk]
                        ai_res = ask_gemini_extract(names_for_ai)
                        
                        for idx, item in enumerate(chunk):
                            ar = ai_res[idx] if idx < len(ai_res) else {}
                            res_save.append([
                                item['SKU'], 
                                ar.get('AI_Brand','Unknown'), 
                                ar.get('AI_Type','Other'), 
                                ar.get('AI_Spec','-'), 
                                ar.get('AI_Tags',''),
                                ar.get('AI_Kind','') 
                            ])
                        time.sleep(4)
                    
                    if res_save:
                        append_to_sheet(res_save)
                        status.update(label="‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!", state="complete")
                        st.balloons()
                        st.cache_data.clear()
                        time.sleep(1)
                        st.rerun()
        else:
            c_a2.button("üîÑ ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î", on_click=lambda: st.cache_data.clear())

        st.divider()
        st.write("üîß **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**")
        
        # ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡∏¢‡∏∞ (‡πÉ‡∏™‡πà key ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á)
        if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (‡∏•‡∏ö AI ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏£‡∏¥‡∏á)", type="secondary", key="btn_cleanup_final"):
            with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î...", expanded=True) as status:
                valid_skus = df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().str.upper().unique()
                df_mem['check_key'] = df_mem['SKU'].astype(str).str.strip().str.upper()
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô Main
                df_mem_clean = df_mem[df_mem['check_key'].isin(valid_skus)].copy()
                # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥
                df_mem_clean = df_mem_clean.drop_duplicates(subset=['check_key'], keep='last')
                
                del df_mem_clean['check_key']
                
                deleted_count = len(df_mem) - len(df_mem_clean)
                
                if deleted_count > 0:
                    status.write(f"üóëÔ∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞/‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥ {deleted_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö")
                    success = overwrite_memory_sheet(df_mem_clean)
                    if success:
                        status.update(label="‚úÖ ‡∏•‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!", state="complete")
                        st.cache_data.clear()
                        time.sleep(2)
                        st.rerun()
                else:
                    status.update(label="‚úÖ ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß", state="complete")

    st.divider()
    
    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ AI ---
    # -------------------------------------------------------------
    # ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ AI (‡∏â‡∏ö‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + Debug Mode)
    # -------------------------------------------------------------
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå Cache ‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï)
    df_search = merge_data(df_main, df_mem)
    
    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå AI_Kind ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ (‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ Cache ‡∏Ñ‡πâ‡∏≤‡∏á ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ô‡∏∞)
    if 'AI_Kind' not in df_search.columns:
        df_search['AI_Kind'] = ''

    col_q1, col_q2 = st.columns([4, 1])
    query2 = col_q1.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô 2 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 8000", key="search_tab2")
    # -------------------------------------------------------------
    # ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ AI (‡∏â‡∏ö‡∏±‡∏ö Universal 100%: ‡πÉ‡∏ä‡πâ‡∏™‡πÄ‡∏Å‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á)
    # -------------------------------------------------------------
    if col_q2.button("‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ AI", type="primary"):
        if query2:
            with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...'):
                cols_ai = ['AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', 'AI_Kind']
                result_json = ask_gemini_filter(query2, cols_ai)
                
                with st.expander("üïµÔ∏è Debug: ‡∏î‡∏π‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î"):
                    st.json(result_json)

                if result_json and 'filters' in result_json:
                    filters = result_json['filters']
                    sort_order = result_json.get('sort_order')
                    
                    final_mask = pd.Series([True] * len(df_search))
                    active_conds = []
                    
                    from collections import defaultdict
                    grouped_filters = defaultdict(list)
                    for f in filters:
                        grouped_filters[f['column']].append(f)
                    
                    # --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà Copy ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á ---
                    try:
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏û‡∏¥‡πà‡∏° AI_Tags ‡πÅ‡∏•‡πâ‡∏ß)
                        text_search_cols = ['AI_Type', 'AI_Kind', 'AI_Tags', 'AI_Brand', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤']

                        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Universal)
                        def extract_numbers_universal(text):
                            try:
                                clean_text = str(text).replace(',', '')
                                nums = re.findall(r'(\d+\.?\d*)', clean_text)
                                if not nums: return []
                                return [float(n) for n in nums if n and n != '.']
                            except: return []

                        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                        def validate_row(extracted_val, conditions):
                            if isinstance(extracted_val, list):
                                for num in extracted_val:
                                    pass_all = True
                                    for cond in conditions:
                                        op = cond['operator']
                                        limit = float(str(cond['value']).replace(',',''))
                                        if op == 'gt' and not (num > limit): pass_all = False; break
                                        if op == 'gte' and not (num >= limit): pass_all = False; break
                                        if op == 'lt' and not (num < limit): pass_all = False; break
                                        if op == 'lte' and not (num <= limit): pass_all = False; break
                                    if pass_all: return True
                                return False
                            else:
                                num = extracted_val
                                for cond in conditions:
                                    op = cond['operator']
                                    limit = float(str(cond['value']).replace(',',''))
                                    if op == 'gt' and not (num > limit): return False
                                    if op == 'gte' and not (num >= limit): return False
                                    if op == 'lt' and not (num < limit): return False
                                    if op == 'lte' and not (num <= limit): return False
                                return True

                        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        for col_ai_suggested, conditions in grouped_filters.items():
                            if col_ai_suggested not in df_search.columns: continue

                            # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç vs ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                            numeric_conds = [f for f in conditions if f['operator'] in ['gt', 'gte', 'lt', 'lte']]
                            choice_conds = [f for f in conditions if f['operator'] not in ['gt', 'gte', 'lt', 'lte']]
                            
                            vals_log = [] # ‡πÄ‡∏Å‡πá‡∏ö Log ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤

                            # -------------------------------------------------
                            # A. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Spec, Price) -> ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡∏£‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏¥‡∏° (Strict)
                            # -------------------------------------------------
                            range_mask = pd.Series([True] * len(df_search))
                            if numeric_conds:
                                if col_ai_suggested == 'AI_Spec':
                                     vals_extracted = df_search[col_ai_suggested].apply(extract_numbers_universal)
                                else:
                                     vals_extracted = df_search[col_ai_suggested].apply(lambda x: extract_numbers_universal(x))
                                
                                range_mask = vals_extracted.apply(lambda x: validate_row(x, numeric_conds))

                            # -------------------------------------------------
                            # B. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° -> ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå + ‡∏Å‡∏±‡∏ô Error ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á üî•
                            # -------------------------------------------------
                            choice_mask = pd.Series([True] * len(df_search))
                            
                            if choice_conds:
                                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (False) ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏´‡∏≤‡πÄ‡∏à‡∏≠
                                choice_mask = pd.Series([False] * len(df_search))
                                
                                for f in choice_conds:
                                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å + ‡∏ï‡∏±‡∏î‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ
                                    target_val = str(f['value']).lower().strip().replace(" ", "")
                                    
                                    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ß‡πà‡∏≤ "‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ"
                                    found_in_any_col = pd.Series([False] * len(df_search))
                                    
                                    for search_col in text_search_cols:
                                        if search_col in df_search.columns:
                                            # üî• ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ Error: .fillna('') ‡∏Ñ‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NaN
                                            col_data_clean = df_search[search_col].fillna('').astype(str).str.lower().str.replace(" ", "")
                                            
                                            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥ (na=False ‡∏Ñ‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡πà‡∏≤ Error)
                                            found_in_any_col |= col_data_clean.str.contains(target_val, na=False)
                                    
                                    # Logic AND: ‡∏ñ‡πâ‡∏≤ User ‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏à‡∏≠‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥
                                    if f is choice_conds[0]:
                                        choice_mask = found_in_any_col
                                    else:
                                        choice_mask &= found_in_any_col
                                        
                                    vals_log.append(f"{target_val}")

                            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (AND Logic ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á ‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡∏∞ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
                            final_mask &= (range_mask & choice_mask)
                            
                            # Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡∏ö‡πâ‡∏≤‡∏ô
                            log_text = ""
                            if numeric_conds: log_text += f"NumCheck({col_ai_suggested}) "
                            if choice_conds:  log_text += f"TextCheck(Match '{'|'.join(vals_log)}' in {text_search_cols})"
                            active_conds.append(log_text)

                    except Exception as e:
                        st.error(f"Filter Logic Error: {e}")
                    # --- ‡∏à‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà Copy ---
                            
                    except Exception as e: st.error(f"Error: {e}")
                else:
                    simple = df_search.astype(str).apply(lambda x: x.str.contains(query2, case=False)).any(axis=1)
                    st.dataframe(df_search[simple], use_container_width=True)
