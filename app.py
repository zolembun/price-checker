import streamlit as st
import pandas as pd
import google.generativeai as genai
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import urllib.parse
from datetime import datetime
import re
import json
import time

# ---------------------------------------------------------
# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î ‡∏´‡πâ‡∏≤‡∏°‡∏¢‡πâ‡∏≤‡∏¢)
# ---------------------------------------------------------
st.set_page_config(page_title="‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤ & AI", page_icon="üí∞", layout="wide")

# ---------------------------------------------------------
# 2. ‡∏£‡∏∞‡∏ö‡∏ö Login
# ---------------------------------------------------------
def check_password():
    def password_entered():
        if st.session_state["password"] == st.secrets["app_password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        st.header("üîí ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
        st.text_input("‡πÉ‡∏™‡πà‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", type="password", on_change=password_entered, key="password")
        return False
    elif not st.session_state["password_correct"]:
        st.header("üîí ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
        st.text_input("‡πÉ‡∏™‡πà‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", type="password", on_change=password_entered, key="password")
        st.error("‚ùå ‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        return False
    else:
        return True

if not check_password():
    st.stop()

# ---------------------------------------------------------
# 3. CSS Styling
# ---------------------------------------------------------
st.markdown("""
<style>
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô */
    .cost-box { 
        background-color: #ffebee; padding: 15px; border-radius: 10px; 
        border: 2px solid #ef5350; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .price-value-cost { font-size: 42px !important; font-weight: 900; color: #c62828; line-height: 1.2;}
    
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≤‡∏¢ */
    .selling-box { 
        background-color: #e8f5e9; padding: 15px; border-radius: 10px; 
        border: 2px solid #66bb6a; text-align: center;
    }
    .price-value-sell { font-size: 42px !important; font-weight: 900; color: #2e7d32; line-height: 1.2;}
    
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• */
    .info-box {
        background-color: #f8f9fa; padding: 15px; border-radius: 10px; border: 1px solid #ddd;
    }
    
    /* ‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î */
    div.stButton > button { width: 100%; border-radius: 8px; font-weight: bold; height: 3em; }
    
    /* ‡∏ã‡πà‡∏≠‡∏ô Header */
    header {visibility: hidden;}
    
    /* Status Widget */
    .stStatusWidget { border-radius: 10px; }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# 4. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Services
# ---------------------------------------------------------
@st.cache_resource
def init_services():
    try:
        service_account_info = st.secrets["gcp_service_account"]
        gemini_key = st.secrets["gemini_api_key"]
        
        scopes = ['https://www.googleapis.com/auth/spreadsheets', 
                  'https://www.googleapis.com/auth/drive.metadata.readonly']
        creds = Credentials.from_service_account_info(service_account_info, scopes=scopes)
        
        sheets_service = build('sheets', 'v4', credentials=creds)
        drive_service = build('drive', 'v3', credentials=creds)
        
        # Config Gemini
        genai.configure(api_key=gemini_key)
        
        # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà Error: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∏‡πà‡∏ô Latest
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
        except:
            # Fallback ‡∏ñ‡πâ‡∏≤ latest ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠ Pro
            model = genai.GenerativeModel('gemini-2.0-flash')
        
        return sheets_service, drive_service, model
    except Exception as e:
        st.error(f"Config Error: {e}")
        return None, None, None

sheets_svc, drive_svc, ai_model = init_services()
if not sheets_svc: st.stop()

try:
    SHEET_URL = st.secrets["sheet_url"]
    SPREADSHEET_ID = SHEET_URL.split('/d/')[1].split('/')[0]
except:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö sheet_url ‡πÉ‡∏ô Secrets")
    st.stop()

# ---------------------------------------------------------
# 5. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ---------------------------------------------------------
@st.cache_data(ttl=600)
def load_data_master():
    try:
        # Metadata
        file_meta = drive_svc.files().get(fileId=SPREADSHEET_ID, fields="name, modifiedTime").execute()
        file_name = file_meta.get('name')
        dt = datetime.strptime(file_meta.get('modifiedTime'), "%Y-%m-%dT%H:%M:%S.%fZ")
        last_update = dt.strftime("%d/%m/%Y %H:%M ‡∏ô.")

        # Main Data
        res_main = sheets_svc.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID, range="A:H").execute()
        vals_main = res_main.get('values', [])
        
        if vals_main:
            df_main = pd.DataFrame(vals_main[1:], columns=vals_main[0])
            for col in ['‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å']:
                if col in df_main.columns:
                    df_main[col] = pd.to_numeric(df_main[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
        else:
            df_main = pd.DataFrame()

        # AI Memory Data
        try:
            res_mem = sheets_svc.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:E").execute()
            vals_mem = res_mem.get('values', [])
            
            if vals_mem and len(vals_mem) > 1:
                headers = vals_mem[0]
                rows = vals_mem[1:]
                fixed_rows = [r + [None]*(len(headers)-len(r)) for r in rows]
                df_mem = pd.DataFrame(fixed_rows, columns=headers)
            else:
                df_mem = pd.DataFrame(columns=['SKU', 'AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags'])
        except:
            df_mem = pd.DataFrame(columns=['SKU', 'AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags'])

        return df_main, df_mem, file_name, last_update

    except Exception as e:
        st.error(f"Load Data Error: {e}")
        return pd.DataFrame(), pd.DataFrame(), "Error", "-"

def append_to_sheet(data_values):
    body = {'values': data_values}
    try:
        sheets_svc.spreadsheets().values().append(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:A", 
            valueInputOption="USER_ENTERED", body=body
        ).execute()
        return True
    except Exception as e: 
        st.error(f"Save Error: {e}")
        return False

def overwrite_memory_sheet(df_new_mem):
    try:
        # 1. ‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô List ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô empty string ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô Error
        df_new_mem = df_new_mem.fillna('') 
        values = [df_new_mem.columns.tolist()] + df_new_mem.values.tolist()
        
        # 2. ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô AI_Memory
        sheets_svc.spreadsheets().values().clear(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A:E"
        ).execute()

        # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏•‡∏á‡πÑ‡∏õ
        body = {'values': values}
        sheets_svc.spreadsheets().values().update(
            spreadsheetId=SPREADSHEET_ID, range="AI_Memory!A1",
            valueInputOption="USER_ENTERED", body=body
        ).execute()
        return True
    except Exception as e:
        st.error(f"Cleanup Error: {e}")
        return False

@st.cache_data(ttl=600)
def merge_data(df_main, df_mem):
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
    if df_mem.empty: return df_main.copy()
    
    # Copy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
    df_main_c = df_main.copy()
    df_mem_c = df_mem.copy()
    
    # üî• ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç 1: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ + ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà + ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Normalize)
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ "sku01" ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö "SKU01" ‡∏´‡∏£‡∏∑‡∏≠ " SKU01 "
    df_main_c['join_key'] = df_main_c['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().str.upper()
    df_mem_c['join_key'] = df_mem_c['SKU'].astype(str).str.strip().str.upper()
    
    # 2. ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà (Merge) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (join_key)
    merged = pd.merge(df_main_c, df_mem_c, on='join_key', how='left')
    
    # 3. ‡∏ñ‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ñ‡πâ‡∏≤ AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NaN)
    cols_to_fix = ['AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', 'AI_Kind'] 
    for col in cols_to_fix:
        if col in merged.columns:
            # fillna('') ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏°‡πà error ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            merged[col] = merged[col].fillna('').astype(str)
    
    # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢ (join_key) ‡∏ó‡∏¥‡πâ‡∏á
    if 'join_key' in merged.columns:
        del merged['join_key']
            
    return merged
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (Force JSON + Debug)
# ---------------------------------------------------------
# ---------------------------------------------------------
# üî• ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏ô‡∏±‡∏Å‡πÅ‡∏Å‡∏∞‡∏£‡∏≠‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡∏û)
# ---------------------------------------------------------
def ask_gemini_extract(names):
    prompt = f"""
    Role: Product Data Expert.
    Task: Extract attributes from product names.
    Input List: {json.dumps(names, ensure_ascii=False)}
    
    Strict Rules:
    1. **Fix Messy Text**: Separate glued words (e.g., "Refrigerator5.2" -> "Refrigerator" + "5.2").
    2. **Standardize Type**: 'AI_Type' MUST be in Thai (e.g., "Refrigerator" -> "‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô", "TV" -> "‡∏ó‡∏µ‡∏ß‡∏µ").
    3. **Extract Kind (NEW)**: 'AI_Kind' is Sub-Category/Feature.
       - Fridge: 1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π, 2 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π, Side by Side
       - Washer: ‡∏ù‡∏≤‡∏ö‡∏ô, ‡∏ù‡∏≤‡∏´‡∏ô‡πâ‡∏≤, 2 ‡∏ñ‡∏±‡∏á
       - Air: Inverter, Fixed Speed, ‡πÅ‡∏Ç‡∏ß‡∏ô, ‡∏ï‡∏¥‡∏î‡∏ú‡∏ô‡∏±‡∏á
    4. **Extract Spec**: Numbers for size/capacity (Q, kg, BTU).
    Output JSON Array ONLY:
   Output JSON Array ONLY:
    [
      {{
        "AI_Brand": "Brand",
        "AI_Type": "Main Category (Thai)",
        "AI_Kind": "Sub Type (Thai) or empty string", 
        "AI_Spec": "Spec",
        "AI_Tags": "Keywords"
      }}
    ]
    """
    
    try:
        response = ai_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            )
        )
        
        # Clean & Parse
        data = json.loads(response.text.strip())
        
        normalized_data = []
        for item in data:
            new_item = {
                "AI_Brand": item.get("AI_Brand") or "Unknown",
                "AI_Type": item.get("AI_Type") or "Other",
                "AI_Kind": item.get("AI_Kind") or "",  # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ
                "AI_Spec": item.get("AI_Spec") or "-",
                "AI_Tags": item.get("AI_Tags") or ""
            }
            # ‡πÅ‡∏õ‡∏•‡∏á Tags ‡πÄ‡∏õ‡πá‡∏ô string ‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
            if isinstance(new_item["AI_Tags"], list):
                new_item["AI_Tags"] = ", ".join(new_item["AI_Tags"])
                
            normalized_data.append(new_item)
            
        return normalized_data

    except Exception as e:
        print(f"AI Error: {e}")
        return []
def ask_gemini_filter(query, columns):
    # ‡∏õ‡∏£‡∏±‡∏ö Prompt ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏¢‡∏Å "‡∏ä‡∏ô‡∏¥‡∏î" (1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π) ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó" (‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô)
    prompt = f"""
    Role: ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Search Engine ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ "{query}" ‡πÄ‡∏õ‡πá‡∏ô JSON Filter
    Columns: {columns}
    
    Instruction (Strict Rules):
    1. **Category vs Kind Strategy (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)**:
       - ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å" (AI_Type) ‡∏Å‡∏±‡∏ö "‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏¢‡πà‡∏≠‡∏¢/‡∏ä‡∏ô‡∏¥‡∏î" (AI_Kind)
       - ‡∏ñ‡πâ‡∏≤ User ‡∏û‡∏¥‡∏°‡∏û‡πå "‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô" ‡πÄ‡∏â‡∏¢‡πÜ -> ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏Ñ‡πà AI_Type="‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô"
       - ‡∏ñ‡πâ‡∏≤ User ‡∏û‡∏¥‡∏°‡∏û‡πå "‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô 1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π" -> ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á AI_Type="‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô" **‡πÅ‡∏•‡∏∞** AI_Kind="1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π"
       - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ã‡∏±‡∏Å‡∏ú‡πâ‡∏≤ ‡∏ù‡∏≤‡∏ö‡∏ô" -> AI_Type="‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ã‡∏±‡∏Å‡∏ú‡πâ‡∏≤", AI_Kind="‡∏ù‡∏≤‡∏ö‡∏ô"
       - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡πÅ‡∏≠‡∏£‡πå Inverter" -> AI_Type="‡πÅ‡∏≠‡∏£‡πå", AI_Kind="Inverter"
    
    2. **Decimal Range Strategy**: 
       - ‡∏´‡∏≤‡∏Å‡πÄ‡∏à‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° (‡πÄ‡∏ä‡πà‡∏ô "5.2 - 7.3 ‡∏Ñ‡∏¥‡∏ß") 
       - **‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (Integer) ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏±‡πâ‡∏ô**
       - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "5.2 - 7.3" -> value: ["5", "6", "7"] 
       
    3. **Price Logic**: 
       - ‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏£‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
       - ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç -> ‡πÉ‡∏ä‡πâ lte (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô), gte (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà)
    
    Output Format (JSON):
    {{
        "filters": [
            {{ "column": "AI_Type", "operator": "contains", "value": "‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô" }},
            {{ "column": "AI_Kind", "operator": "contains", "value": "1 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π" }},
            {{ "column": "AI_Spec", "operator": "contains", "value": "5" }}
        ],
        "sort_order": "asc"
    }}
    """
    try:
        res = ai_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            )
        )
        return json.loads(res.text.strip())
    except: return None
# ---------------------------------------------------------
# 6. MAIN APP UI (TABS)
# ---------------------------------------------------------

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df_main, df_mem, file_name, last_update = load_data_master()

st.title("üí∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ & AI")
st.caption(f"üìÇ ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {file_name} | üïí ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {last_update}")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á TAB ‡πÄ‡∏°‡∏ô‡∏π
tab1, tab2 = st.tabs(["üè† ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (Code/Name)", "ü§ñ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (AI Search)"])

# =========================================================
# TAB 1: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
# =========================================================
with tab1:
    st.info("üí° ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏π‡πâ '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô' ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô")
    
    query1 = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô", placeholder="‡πÄ‡∏ä‡πà‡∏ô rt20, parsr5lae (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ç‡∏µ‡∏î)", key="search_tab1")
    
    if query1:
        match_index = -1
        found_by = ""
        
        query_clean = clean_text(query1)
        sku_clean_series = df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).apply(clean_text)
        desc_clean_series = df_main['‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).apply(clean_text)
        
        sku_matches = df_main[sku_clean_series.str.contains(query_clean, na=False)]
        if not sku_matches.empty:
            match_index = sku_matches.index[0]
            found_by = "‚ö° ‡πÄ‡∏à‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"
        else:
            desc_matches = df_main[desc_clean_series.str.contains(query_clean, na=False)]
            if not desc_matches.empty:
                match_index = desc_matches.index[0]
                found_by = "üîé ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
            else:
                keywords = list(filter(None, re.split(r'[^a-zA-Z0-9]', query1)))
                if not keywords: keywords = [query1]
                candidates = df_main[df_main.astype(str).apply(lambda x: any(k.lower() in x.lower() for k in keywords), axis=1)]
                
                if candidates.empty: search_pool = df_main.sample(min(len(df_main), 15))
                else: search_pool = candidates.head(30)
                
                prod_str = search_pool[['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤']].to_string(index=True)
                with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡∏∞‡∏•‡∏≤‡∏¢‡πÅ‡∏ó‡∏á...'):
                    try:
                        res = ai_model.generate_content(f"‡∏´‡∏≤ index ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö '{query1}' ‡∏à‡∏≤‡∏Å:\n{prod_str}\n‡∏ï‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç index. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ -1")
                        match_index = int(res.text.strip())
                        found_by = "ü§ñ AI ‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö"
                    except: match_index = -1

        if match_index != -1 and match_index in df_main.index:
            item = df_main.loc[match_index]
            cost = item.get('‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', 0)
            stock = item.get('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å', 0)
            mid = item.get('‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '-')
            name = item.get('‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '-')
            brand = item.get('‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠', '-')

            st.success(f"{found_by}: {name}")
            
            target_margin = 12
            sell_price = cost * (1 + (target_margin/100))
            profit = sell_price - cost

            c1, c2, c3 = st.columns([1.3, 1.3, 1])
            with c1:
                st.markdown(f"""<div class="cost-box"><div style="color:#555;font-weight:bold;">üî¥ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô</div><div class="price-value-cost">{cost:,.0f}</div></div>""", unsafe_allow_html=True)
            with c2:
                st.markdown(f"""<div class="selling-box"><div style="color:#555;font-weight:bold;">üü¢ ‡∏Ç‡∏≤‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (+{target_margin}%)</div><div class="price-value-sell">{sell_price:,.0f}</div><div style="color:#1b5e20;">‡∏Å‡∏≥‡πÑ‡∏£ {profit:,.0f} ‡∏ö‡∏≤‡∏ó</div></div>""", unsafe_allow_html=True)
            with c3:
                st.markdown(f"""<div class="info-box"><b>üÜî ‡∏£‡∏´‡∏±‡∏™:</b> {mid}<br><b>üì¶ ‡∏™‡∏ï‡πâ‡∏≠‡∏Å:</b> {stock}<br><b>üè∑Ô∏è ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠:</b> {brand}</div>""", unsafe_allow_html=True)
                
                st.write("")
                g_q = urllib.parse.quote(name)
                st.link_button("üåê ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ/‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Google", f"https://www.google.com/search?q={g_q}", use_container_width=True)

            st.divider()
            with st.expander("‡∏î‡∏π‡∏ï‡∏≤‡∏£‡∏≤‡∏á Margin (3% - 30%)", expanded=True):
                margins = [3, 5, 8, 10, 12, 15, 18, 25, 30]
                p_data = [{"‡∏Å‡∏≥‡πÑ‡∏£ %": f"{m}%", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≤‡∏¢": f"{cost*(1+m/100):,.0f}", "‡∏Å‡∏≥‡πÑ‡∏£ (‡∏ö‡∏≤‡∏ó)": f"{(cost*(1+m/100))-cost:,.0f}"} for m in margins]
                st.dataframe(pd.DataFrame(p_data), hide_index=True, use_container_width=True)

            st.divider()
            st.subheader("üõí ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏Ç‡πà‡∏á (Hot Search)")
            default_search = re.sub(r'[\u0E00-\u0E7F]', '', str(mid)).strip('-').strip()
            final_kw = st.text_input("üéØ ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:", value=default_search, key="hot_kw")
            
            if final_kw:
                enc = urllib.parse.quote(final_kw.strip())
                stores = [
                    {"name": "Shopee", "url": f"https://shopee.co.th/search?keyword={enc}"},
                    {"name": "Lazada", "url": f"https://www.lazada.co.th/catalog/?q={enc}"},
                    {"name": "HomePro", "url": f"https://www.homepro.co.th/search?q={enc}"},
                    {"name": "PowerBuy", "url": f"https://www.powerbuy.co.th/th/search/{enc}"},
                    {"name": "ThaiWatsadu", "url": f"https://www.thaiwatsadu.com/th/search/{enc}"},
                    {"name": "Big C", "url": f"https://www.bigc.co.th/search?q={enc}"},
                    {"name": "Global", "url": f"https://globalhouse.co.th/search?keyword={enc}"},
                    {"name": "Makro", "url": f"https://www.makro.pro/c/search?q={enc}"},
                    {"name": "Dohome", "url": f"https://www.dohome.co.th/search?q={enc}"}
                ]
                
                cols = st.columns(2)
                for i, s in enumerate(stores):
                    with cols[i%2]: st.link_button(f"üîç {s['name']}", s['url'], use_container_width=True)
        else:
            if query1: st.warning(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤: '{query1}'")

# =========================================================
# TAB 2: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ AI
# =========================================================
with tab2:
    st.info("üí° ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô '‡∏ó‡∏µ‡∏ß‡∏µ Samsung ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏´‡∏°‡∏∑‡πà‡∏ô', '‡πÅ‡∏≠‡∏£‡πå inverter'")
    
    processed_skus = df_mem['SKU'].astype(str).str.strip().tolist() if not df_mem.empty else []
    new_items_df = df_main[~df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().isin(processed_skus)]
    new_count = len(new_items_df)
    
    with st.expander(f"‚öôÔ∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏≠‡∏á AI ({len(df_mem)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß)"):
        c_a1, c_a2 = st.columns([3, 1])
        c_a1.write(f"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: **{new_count}** ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏≠‡∏ô AI (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Indent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢) ---
        if new_count > 0:
            if c_a2.button("üöÄ ‡∏™‡∏≠‡∏ô AI ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ô‡∏µ‡πâ", type="primary"):
                with st.status("ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ...", expanded=True) as status:
                    
                    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏ô‡∏¥‡∏î
                    if '‡∏ä‡∏ô‡∏¥‡∏î' not in new_items_df.columns:
                        new_items_df['‡∏ä‡∏ô‡∏¥‡∏î'] = ''
                        
                    # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ä‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏¥‡∏°)
                    to_proc = new_items_df[['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏ä‡∏ô‡∏¥‡∏î']].rename(
                        columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤':'SKU', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤':'Name', '‡∏ä‡∏ô‡∏¥‡∏î':'Original_Kind'}
                    ).to_dict('records')

                    BATCH = 10
                    res_save = []
                    total_batches = (len(to_proc) // BATCH) + 1
                    
                    for i in range(0, len(to_proc), BATCH):
                        chunk = to_proc[i:i+BATCH]
                        status.write(f"Batch {(i//BATCH)+1}/{total_batches} ({len(chunk)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)...")
                        
                        names_for_ai = [f"{x['Name']} {x['Original_Kind']}" for x in chunk]
                        ai_res = ask_gemini_extract(names_for_ai)
                        
                        for idx, item in enumerate(chunk):
                            ar = ai_res[idx] if idx < len(ai_res) else {}
                            res_save.append([
                                item['SKU'], 
                                ar.get('AI_Brand','Unknown'), 
                                ar.get('AI_Type','Other'), 
                                ar.get('AI_Spec','-'), 
                                ar.get('AI_Tags',''),
                                ar.get('AI_Kind','') 
                            ])
                        time.sleep(4)
                    
                    if res_save:
                        append_to_sheet(res_save)
                        status.update(label="‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!", state="complete")
                        st.balloons()
                        st.cache_data.clear()
                        time.sleep(1)
                        st.rerun()
        else:
            c_a2.button("üîÑ ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î", on_click=lambda: st.cache_data.clear())

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡∏¢‡∏∞ (‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Expander ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô) ---
        st.divider()
        st.write("üîß **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**")
        
        if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (‡∏•‡∏ö AI ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏£‡∏¥‡∏á)", type="secondary"):
            with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î...", expanded=True) as status:
                valid_skus = df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().str.upper().unique()
                df_mem['check_key'] = df_mem['SKU'].astype(str).str.strip().str.upper()
                
                df_mem_clean = df_mem[df_mem['check_key'].isin(valid_skus)].copy()
                df_mem_clean = df_mem_clean.drop_duplicates(subset=['check_key'], keep='last')
                del df_mem_clean['check_key']
                
                deleted_count = len(df_mem) - len(df_mem_clean)
                
                if deleted_count > 0:
                    status.write(f"üóëÔ∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ {deleted_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö")
                    success = overwrite_memory_sheet(df_mem_clean)
                    if success:
                        status.update(label="‚úÖ ‡∏•‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!", state="complete")
                        st.cache_data.clear()
                        time.sleep(2)
                        st.rerun()
                else:
                    status.update(label="‚úÖ ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß", state="complete")

            # ========================================================
        # üî• ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡∏¢‡∏∞ (‡∏ß‡∏≤‡∏á‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô expander)
        # ========================================================
        st.divider()
        st.write("üîß **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**")
        
        if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (‡∏•‡∏ö AI ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏£‡∏¥‡∏á)", type="secondary"):
            with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î...", expanded=True) as status:
                # 1. ‡∏´‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Main
                valid_skus = df_main['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤'].astype(str).str.strip().str.upper().unique()
                
                # 2. ‡∏Å‡∏£‡∏≠‡∏á df_mem ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô valid_skus
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á column ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                df_mem['check_key'] = df_mem['SKU'].astype(str).str.strip().str.upper()

                # -------------------------------------------------------
                # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ: ‡∏Å‡∏£‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á AND ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà SKU ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
                # -------------------------------------------------------
                
                # Step A: ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô Main (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
                df_mem_clean = df_mem[df_mem['check_key'].isin(valid_skus)].copy()
                
                # Step B: ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà!) 
                # keep='last' ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ã‡πâ‡∏≥ ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏ß‡πà‡∏≤)
                df_mem_clean = df_mem_clean.drop_duplicates(subset=['check_key'], keep='last')
                
                # -------------------------------------------------------
                
                # ‡∏Ñ‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
                df_mem_clean = df_mem[df_mem['check_key'].isin(valid_skus)].copy()
                
                # ‡∏•‡∏ö column ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏¥‡πâ‡∏á
                del df_mem_clean['check_key']
                
                deleted_count = len(df_mem) - len(df_mem_clean)
                
                if deleted_count > 0:
                    status.write(f"üóëÔ∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ {deleted_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö")
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô overwrite ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô
                    success = overwrite_memory_sheet(df_mem_clean) 
                    if success:
                        status.update(label=f"‚úÖ ‡∏•‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(df_mem_clean)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)", state="complete")
                        st.cache_data.clear()
                        time.sleep(2)
                        st.rerun()
                else:
                    status.update(label="‚úÖ ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏ö", state="complete")

    st.divider()
    
    df_search = merge_data(df_main, df_mem)
    
    col_q1, col_q2 = st.columns([4, 1])
    query2 = col_q1.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô 2 ‡∏õ‡∏£‡∏∞‡∏ï‡∏π ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 8000", key="search_tab2")
   # ‡∏ß‡∏≤‡∏á‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î: query2 = col_q1.text_input(...)
if col_q2.button("‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ AI", type="primary"):
        with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...'):
                # 1. ‡πÄ‡∏û‡∏¥‡πà‡∏° AI_Kind ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                cols_ai = ['AI_Brand', 'AI_Type', 'AI_Spec', 'AI_Tags', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', 'AI_Kind']
                result_json = ask_gemini_filter(query2, cols_ai)
                
                if result_json and 'filters' in result_json:
                    filters = result_json['filters']
                    sort_order = result_json.get('sort_order')
                    
                    final_mask = pd.Series([True] * len(df_search))
                    active_conds = []
                    
                    from collections import defaultdict
                    grouped_filters = defaultdict(list)
                    for f in filters:
                        grouped_filters[f['column']].append(f)
                    
                    try:
                        for col, conditions in grouped_filters.items():
                            if col not in df_search.columns: continue
                            
                            col_mask = pd.Series([False] * len(df_search))
                            vals_log = []
                            
                            for f in conditions:
                                op = f['operator']
                                raw_val = f['value']
                                values_list = raw_val if isinstance(raw_val, list) else [raw_val]
                                
                                for val in values_list:
                                    if col == '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢':
                                        s_val = pd.to_numeric(df_search[col], errors='coerce').fillna(0)
                                        val = float(val)
                                    else:
                                        s_val = df_search[col].astype(str)
                                        # ‡∏ï‡∏±‡∏î‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° .0
                                        try:
                                            if isinstance(val, (int, float)) and val == int(val):
                                                val = str(int(val))
                                            else:
                                                val = str(val)
                                        except: val = str(val)

                                    if op == 'contains': sub_mask = s_val.str.contains(val, case=False, na=False)
                                    elif op == 'equals': sub_mask = (s_val == val)
                                    elif op == 'gt': sub_mask = (s_val > val)
                                    elif op == 'gte': sub_mask = (s_val >= val)
                                    elif op == 'lt': sub_mask = (s_val < val)
                                    elif op == 'lte': sub_mask = (s_val <= val)
                                    else: sub_mask = pd.Series([False] * len(df_search))
                                    
                                    col_mask |= sub_mask
                                    vals_log.append(f"{val}")
                            
                            final_mask &= col_mask
                            active_conds.append(f"{col}: {'|'.join(vals_log)}")
                        
                        results = df_search[final_mask]
                        
                        if not results.empty and sort_order:
                            if sort_order == 'asc': results = results.sort_values(by='‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', ascending=True)
                            elif sort_order == 'desc': results = results.sort_values(by='‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', ascending=False)

                        if not results.empty:
                            st.success(f"‚úÖ ‡∏û‡∏ö {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                            
                            # 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° AI_Kind ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                            st.dataframe(
                                results[['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å', 'AI_Brand', 'AI_Spec', 'AI_Kind']],
                                column_config={
                                    "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢": st.column_config.NumberColumn("‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏∏‡∏ô", format="‡∏ø%d"), 
                                    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å": st.column_config.ProgressColumn("‡∏™‡∏ï‡πâ‡∏≠‡∏Å", format="%d", max_value=int(df_search['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ï‡πâ‡∏≠‡∏Å'].max()))
                                },
                                use_container_width=True, hide_index=True
                            )
                        else: 
                            st.warning(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: {'; '.join(active_conds)})")
                            
                    except Exception as e: st.error(f"Error: {e}")
                else:
                    simple = df_search.astype(str).apply(lambda x: x.str.contains(query2, case=False)).any(axis=1)
                    st.dataframe(df_search[simple], use_container_width=True)
